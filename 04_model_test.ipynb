{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model accuracy on test data\n",
    "*Split from main notebooks to limit notebook size and also for organisational puproses\n",
    "\n",
    "Now that we have trained and tuned two models, we have to pick a better one to continue with deploy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as rt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Coast', 'Desert', 'Forest', 'Glacier', 'Mountain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing data paths and lists\n",
    "\n",
    "data_path = \"./data/Testing Data\"\n",
    "\n",
    "coast_names = [name for name in os.listdir(data_path + \"/Coast\") if os.path.isfile(os.path.join(data_path + \"/Coast\", name))]\n",
    "desert_names = [name for name in os.listdir(data_path + \"/Desert\") if os.path.isfile(os.path.join(data_path + \"/Desert\", name))]\n",
    "forest_names = [name for name in os.listdir(data_path + \"/Forest\") if os.path.isfile(os.path.join(data_path + \"/Forest\", name))]\n",
    "glacier_names = [name for name in os.listdir(data_path + \"/Glacier\") if os.path.isfile(os.path.join(data_path + \"/Glacier\", name))]\n",
    "mountain_names = [name for name in os.listdir(data_path + \"/Mountain\") if os.path.isfile(os.path.join(data_path + \"/Mountain\", name))]\n",
    "\n",
    "all_names = [coast_names + desert_names + forest_names + glacier_names + mountain_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to transform image data to form model can accept\n",
    "\n",
    "def image_transformer(path: str, size: int) -> np.ndarray:\n",
    "    image = Image.open(path)\n",
    "    image = image.resize((size, size))\n",
    "    \n",
    "    image = np.array(image)\n",
    "    image = image.transpose(2,0,1).astype(np.float32)\n",
    "    image /= 255\n",
    "\n",
    "    image = image[None, ...]\n",
    "\n",
    "    return image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(yes, this is the most inelegant solution there probably is to do this. Need to optimise, will do later.)\n",
    "\n",
    "(ideally this sort of testing should've been done with data loaders before saving and exporting model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that tediously brute-force tests the model...\n",
    "\n",
    "def model_test_run(session, inputs, outputs):\n",
    "    preds = []\n",
    "\n",
    "    for item in coast_names:\n",
    "        item = image_transformer(f\"{data_path}/Coast/{item}\", 224)\n",
    "        results = session.run([outputs], {inputs: item})[0]\n",
    "        label = labels[np.argmax(results)]\n",
    "        if label == \"Coast\":\n",
    "            preds.append(True)\n",
    "\n",
    "    for item in desert_names:\n",
    "        item = image_transformer(f\"{data_path}/Desert/{item}\", 224)\n",
    "        results = session.run([outputs], {inputs: item})[0]\n",
    "        label = labels[np.argmax(results)]\n",
    "        if label == \"Desert\":\n",
    "            preds.append(True)\n",
    "\n",
    "    for item in forest_names:\n",
    "        item = image_transformer(f\"{data_path}/Forest/{item}\", 224)\n",
    "        results = session.run([outputs], {inputs: item})[0]\n",
    "        label = labels[np.argmax(results)]\n",
    "        if label == \"Forest\":\n",
    "            preds.append(True)\n",
    "\n",
    "    for item in glacier_names:\n",
    "        item = image_transformer(f\"{data_path}/Glacier/{item}\", 224)\n",
    "        results = session.run([outputs], {inputs: item})[0]\n",
    "        label = labels[np.argmax(results)]\n",
    "        if label == \"Glacier\":\n",
    "            preds.append(True)\n",
    "\n",
    "    for item in mountain_names:\n",
    "        item = image_transformer(f\"{data_path}/Mountain/{item}\", 224)\n",
    "        results = session.run([outputs], {inputs: item})[0]\n",
    "        label = labels[np.argmax(results)]\n",
    "        if label == \"Mountain\":\n",
    "            preds.append(True)\n",
    "\n",
    "    return len(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and checking resnet34 model\n",
    "\n",
    "model_res34 = onnx.load(\"models/landscape_model_resnet34.onnx\")\n",
    "onnx.checker.check_model(model_res34)\n",
    "\n",
    "# Despite the name this returns a very un-human readable graph, hence I left it commented. Uncomment if you want to, it works just fine. \n",
    "# onnx.helper.printable_graph(model_res34.graph) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and checking resnet50 model\n",
    "\n",
    "model_res50 = onnx.load(\"models/landscape_model_resnet50.onnx\")\n",
    "onnx.checker.check_model(model_res50)\n",
    "\n",
    "# onnx.helper.printable_graph(model_res50.graph) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('input', 'output', [1, 3, 224, 224])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting inference session for resnet34 model\n",
    "\n",
    "res34_session = rt.InferenceSession(\"models\\landscape_model_resnet34.onnx\")\n",
    "res34_inputs = res34_session.get_inputs()[0].name\n",
    "res34_outputs = res34_session.get_outputs()[0].name\n",
    "dims = res34_session.get_inputs()[0].shape\n",
    "\n",
    "res34_inputs, res34_outputs, dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet34 model predictions on test data\n",
    "\n",
    "resnet_34_preds = model_test_run(res34_session, res34_inputs, res34_outputs)\n",
    "accuracy_res34 = resnet_34_preds / len(all_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('input', 'output', [1, 3, 224, 224])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting inference session for resnet50 model\n",
    "\n",
    "res50_session = rt.InferenceSession(\"models\\landscape_model_resnet50.onnx\")\n",
    "res50_inputs = res50_session.get_inputs()[0].name\n",
    "res50_outputs = res50_session.get_outputs()[0].name\n",
    "res50_dims = res50_session.get_inputs()[0].shape\n",
    "\n",
    "res50_inputs, res50_outputs, dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet34 model predictions on test data\n",
    "\n",
    "resnet_50_preds = model_test_run(res50_session, res50_inputs, res50_outputs)\n",
    "accuracy_res50 = resnet_50_preds / len(all_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of resnet34 model on test dataset: 0.352\n",
      "Accuracy of resnet50 model on test dataset: 0.968\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy of resnet34 model on test dataset: {accuracy_res34}\")\n",
    "print(f\"Accuracy of resnet50 model on test dataset: {accuracy_res50}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason there is a huge loss of accuracy in the resnet34 model. I actually suspect some sort of error in exporting and saving the model given the high validation accuracy, but I don't have the time to go back and check that. As I intended to use the resnet50 model anyway (due to higher val accuracy, and more tuning), we will just go ahead with that model as planned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlzoomcamp_capstone2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9a6a818fa93767133a3a548321a26a54bd13c62c638e255905e1ef5a7151df0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
